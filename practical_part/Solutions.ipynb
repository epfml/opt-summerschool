{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell (shift + enter) to start.\n",
    "import tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is just like NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(12).reshape(4, 3)\n",
    "print(x)\n",
    "third_row = x[2, :]\n",
    "print(\"Third row:\", third_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(12).reshape(4, 3)\n",
    "print(x)\n",
    "third_row = x[2, :]\n",
    "print(\"Third row:\", third_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correspondence is not exactly 1–1, but there exist handy [translation tables](https://github.com/wkentaro/pytorch-for-numpy-users) for NumPy users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be allocated on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(12, device='cuda').reshape(4, 3) # you will get an error if GPU is not available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or moved there from the cpu, (and back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(2).to('cuda') # you will get an error if GPU is not available\n",
    "b = torch.tensor(3).to('cuda')\n",
    "c = a + b  # This is done by your GPU\n",
    "c_on_cpu = c.to('cpu')\n",
    "print(c_on_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like TensorFlow, PyTorch can track a tensor's computation history, and automatically compute gradients w.r.t. input tensors. This is enabled with `requires_grad=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's use autodiff to find the slope of a x + b\n",
    "x = torch.randn(1, requires_grad=True)  # sample from a standard normal distribution\n",
    "print(\"x =\", x)\n",
    "\n",
    "y = 2 * x + 3\n",
    "print(\"y =\", y)\n",
    "\n",
    "# Differentiate y w.r.t. all inputs that have `requires_grad=True`:\n",
    "y.backward()\n",
    "\n",
    "# `.backard()` *adds* the gradients to `variable`.grad for each of the inputs\n",
    "print(\"grad x =\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding autodiff for parameter updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When updating model parameters in SGD, you want to avoid tracking that update for automatic differentiation. \n",
    "You can do this by updating the *raw storage* of the parameter with `tensor.data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normally, every change to a parameter is recorded:\n",
    "x = torch.ones(2, requires_grad=True)\n",
    "x = x + 2.0\n",
    "print(\"Tracked addition:  \", x)\n",
    "\n",
    "# This updates the tensor 'secretly' by updating the tensor's raw storage:\n",
    "# This is how we do model parameter updates\n",
    "x = torch.ones(2, requires_grad=True)\n",
    "x.data = x + 2.0\n",
    "print(\"Untracked addition:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first neural network in PyTorch\n",
    "This section is based on [Pytorch tutorial](https://github.com/ahug/amld-pytorch-workshop) from Andreas Hug and Evann Courdier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a neural network, you need to define a PyTorch module with two methods: `__init__` and `forward`. You define a simple linear model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \"\"\"\"\n",
    "        In the `__init__` method, you define all the layers with trainable parameters.\n",
    "        The order does not matter.\n",
    "        \"\"\"\n",
    "        super(MySuperSimpleNetwork, self).__init__()  # Mandatory to call Super\n",
    "        self.linear = torch.nn.Linear(input_size, num_classes)  # Define one Linear layer\n",
    "    \n",
    "    def forward(self, network_input):\n",
    "        \"\"\"\n",
    "        In the `forward` method, you run the network, composing the building blocks\n",
    "        defined in `__init__` in the order you want.\n",
    "        \"\"\"\n",
    "        return self.linear(network_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a more realistic network for image classification. \n",
    "Your first exercise is to implement a simple multilayer perceptron (MLP) with two fully-connected hidden layers and *ReLU* nonlinearities between them.\n",
    "\n",
    "Use the following number of 'neurons' per layer:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Layer</th>\n",
    "            <th># of neurons</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><th>Input</th><td><code>input_size</code></td></tr>\n",
    "        <tr><th>1st hidden layer</th><td>75</td></tr>\n",
    "        <tr><th>2nd hidden layer</th><td>50</td></tr>\n",
    "        <tr><th>Output layer</th><td><code>num_classes</code></td></tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MyMultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        ###### INSERT YOUR CODE HERE #########\n",
    "        self.linear_1 = torch.nn.Linear(input_size, 75)\n",
    "        self.linear_2 = torch.nn.Linear(75, 50)\n",
    "        self.linear_3 = torch.nn.Linear(50, num_classes)\n",
    "        ######################################\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size(0), -1)\n",
    "        ###### INSERT YOUR CODE HERE #########\n",
    "        # Hint: the 'ReLU' function is torch.nn.functional.relu\n",
    "        out = torch.nn.functional.relu(self.linear_1(out))\n",
    "        out = torch.nn.functional.relu(self.linear_2(out))\n",
    "        ######################################\n",
    "        out = self.linear_3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CIFAR10 dataset \n",
    "\n",
    "We will work with 2 class subset of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. This dataset contains 32x32 color images of 'airplanes' and 'automobiles' with their labels. There are 5000 of these images per category in the training set, and 1000 each in the test set. We use only 2 out of 10 CIFAR10 classes to save time during the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide an implementation of the 2-class subset of CIFAR10.\n",
    "# If you wanted to use the full dataset, use torchvision.datasets.CIFAR10\n",
    "from tutorial import TwoClassCIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load both the training- and test sets into memory\n",
    "training_set = TwoClassCIFAR(train=True, transform=torchvision.transforms.ToTensor())\n",
    "test_set = TwoClassCIFAR(train=False, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# And define loaders that allow us to iterate over the dataset in batches\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=250, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image of CIFAR10 consists has 32x32 pixels with 3 color channels (red, green, and blue)\n",
    "# With this information, the number of inputs to the neural network follows as\n",
    "input_size = 3 * 32 * 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD \"by hand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset and a model set up, it is now time to train the model.\n",
    "\n",
    "We will optimize the multi-layer perceptron \"by hand\" using the gradient descent algorithm. It is good practice to use PyTorch' built-in optimizers, but we will first implement SGD from basic building blocks for educational purposes :)\n",
    "\n",
    "As a reminder, the update step of the algorithm is:\n",
    "$$x_{t+1} = x_{t} - \\lambda \\nabla_x f (x_t).$$\n",
    "\n",
    "This is equivalent to the momentum discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedures are structured as:\n",
    "* loop over epochs\n",
    "* loop over training data \n",
    "* forward pass (make a prediction)\n",
    "* zero gradients: reset the gradient buffers from the previous iteration to zero\n",
    "* backward pass (compute the mini-batch gradient)\n",
    "* update the model parameters\n",
    "\n",
    "**Exercise:** Implement Stochastic Gradient Descent with a constant learning rate.\n",
    "\n",
    "Note:\n",
    "- The gradient information $\\nabla_x f (x)$ for a parameter `param` is stored in `param.grad` after running `loss.backward()`.\n",
    "- We need to update the parameters by changing the raw storage with `param.data = ...`. This prevents autograd from tracking this change.\n",
    "- `model.parameters()` gives the list of all parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model: torch.nn.Module, num_epochs: int, lr: float):\n",
    "    plot = tutorial.LivePlot(legend=[\"Training loss\"])  # Track trainig errors\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)  # This moves all model parameters to the device (GPU if available)\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # The loss function\n",
    "    \n",
    "    losses = []  # Keep track of the training loss values that we encounter\n",
    "    \n",
    "    # Passes over the whole training set\n",
    "    for epoch in range(num_epochs):\n",
    "        cumulative_loss = 0.0\n",
    "        \n",
    "        # Loop over mini-batches\n",
    "        for (inputs, labels) in training_loader:\n",
    "            # `inputs` has shape (batch size, rgb, height, width)\n",
    "            # `labels` has shape (batch size)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # pre-softmax probabilities, shape: (batch size, num_classes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Resetting the gradient buffers from the last iteration\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Gradient computation\n",
    "            loss.backward()\n",
    "\n",
    "            # Parameters update with SGD step\n",
    "            ###### INSERT YOUR CODE HERE #########\n",
    "            for param in model.parameters():\n",
    "                param.data = param - lr * param.grad\n",
    "            ######################################\n",
    "\n",
    "            cumulative_loss += loss.item()  # .item() transforms a torch Tensor with a single number to a 'float'\n",
    "\n",
    "        plot.add_point(epoch, cumulative_loss)\n",
    "        losses.append(cumulative_loss)\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "model = MyMultilayerPerceptron(input_size, 2)\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "sgd_losses = sgd(model, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement function to compute the accuracy on the test set. You can use `torch.argmax(..., dim=1)` to translate class probabilities (the model output) to predicted labels. \n",
    "\n",
    "For two torch tensors `a` and `b`, `a.eq(b)` gives a tensor of elementwise comparison of these two tensors (1 if corresponding entries are equal and 0 if not). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader):\n",
    "    accuracies_per_batch = []\n",
    "    for (inputs, labels) in data_loader:\n",
    "        # `inputs` has shape (batch size, rgb, height, width)\n",
    "        # `labels` has shape (batch size)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        ###### INSERT YOUR CODE HERE #########\n",
    "        # predict with forward pass\n",
    "        # get predicted labels\n",
    "        # compare with true labels\n",
    "        # get mean of number of correct preditcions\n",
    "        prediction = model(inputs)\n",
    "        predicted_labels = torch.argmax(prediction, dim=1)\n",
    "        correct_predictions = labels.eq(predicted_labels)\n",
    "        batch_accuracy = correct_predictions.sum().float() / correct_predictions.nelement()\n",
    "        accuracies_per_batch.append(batch_accuracy.item())\n",
    "        ######################################\n",
    "        \n",
    "    return np.mean(accuracies_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", accuracy(model, training_loader))\n",
    "print(\"Test accuracy:\", accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement SGD with momentum\n",
    "\n",
    "Recall that the update step of the SGD with momentum is:\n",
    "$$m_{t+1} = \\beta m_{t} + \\nabla_x f (x_t)$$\n",
    "$$x_{t+1} = x_{t} - \\lambda m_{t + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_with_momentum(model, num_epochs, lr, mom=0.9):\n",
    "    plot = tutorial.LivePlot(legend=['Training loss'])\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # Create a momentum buffer for each parameter, initialized at zero\n",
    "    momentum_buffers = []\n",
    "    for param in model.parameters():\n",
    "        momentum_buffers.append(torch.zeros_like(param))\n",
    "        \n",
    "    # Passes over the whole dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        cumulative_loss = 0.0\n",
    "        \n",
    "        # Loop over batches in the dataset\n",
    "        for (inputs, labels) in training_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Run the model\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Reset gradient buffers\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Compute the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # SGD step with Momentum\n",
    "            ###### INSERT YOUR CODE HERE #########\n",
    "            for param, momentum in zip(model.parameters(), momentum_buffers):\n",
    "                momentum.data = mom * momentum.data + param.grad\n",
    "                param.data = param - lr * momentum\n",
    "            ######################################\n",
    "\n",
    "            cumulative_loss += loss.item()\n",
    "            \n",
    "        plot.add_point(epoch, cumulative_loss)\n",
    "        losses.append(cumulative_loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyMultilayerPerceptron(input_size, 2)\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "sgd_mom_losses = sgd_with_momentum(model, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", accuracy(model, training_loader))\n",
    "print(\"Test accuracy:\", accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing SGD with SGD+Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(sgd_losses, label=\"SGD\")\n",
    "plt.plot(sgd_mom_losses, label=\"SGD with Momentum\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one is better? Why should you be skeptical about this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoping to improve our classification performance, we will try LeNet-5—a network that is deeper than our toy-MLP.\n",
    "![](https://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv_net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 6, kernel_size=(5, 5)),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            torch.nn.Conv2d(6, 16, kernel_size=(5, 5)),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            torch.nn.Conv2d(16, 120, kernel_size=(5, 5)),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fully_connected = torch.nn.Sequential(\n",
    "            torch.nn.Linear(120, 84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84, 2),\n",
    "            torch.nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        output = self.conv_net(imgs)\n",
    "        output = output.view(imgs.shape[0], -1)  # imgs.shape[0] == batch_size\n",
    "        output = self.fully_connected(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch's built-in optimizers\n",
    "Luckily, you don't normally need to implement your own optimizers from scratch. PyTorch provides the most common optimization algorithms encapsulated into \"optimizer classes\".  \n",
    "An optimizer is a useful object that automatically loops through all the numerous parameters of your model and performs the (potentially complex) update step for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the most commonly used optimizers. Each of them have its specific parameters that you can check on the [PyTorch documentation](https://pytorch.org/docs/master/optim.html#algorithms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "optimizer = torch.optim.SGD(parameters, lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "optimizer = torch.optim.Adadelta(parameters, lr=0.01)\n",
    "optimizer = torch.optim.Adagrad(parameters, lr=0.01)\n",
    "optimizer = torch.optim.RMSprop(parameters, lr=0.01)\n",
    "optimizer = torch.optim.LBFGS(parameters, lr=0.01)\n",
    "```\n",
    "\n",
    " and there is more ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use oneo f these optimizers!\n",
    "\n",
    "You will need 2 new functions:\n",
    "- `optimizer.zero_grad()` : This function sets the gradient of the parameters (x here) to 0 (otherwise it will get accumulated)\n",
    "- `optimizer.step()` :  This function applies an update step. You run this after computing gradients with `loss.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, num_epochs: int, data_loader, optimizer: torch.optim.Optimizer):\n",
    "    plot = tutorial.LivePlot(legend=[\"Training loss\"])\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define the Loss function and Optimizer that you want to use\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "   \n",
    "    # Outter training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Inner training loop\n",
    "        cumulative_loss = 0.0\n",
    "        for (inputs, labels) in data_loader:\n",
    "            # `inputs` has shape (batch size, rgb, height, width)\n",
    "            # `labels` has shape (batch size)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Do Forward -> Loss Computation -> Backward -> Optimization\n",
    "            # And don't forget zero the gradient buffer!\n",
    "            ######### INSERT YOUR CODE HERE #########\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #########################################\n",
    "\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "        plot.add_point(epoch, cumulative_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of an optimizer can heavily depend on the choice of learning rates, momentum parameter, batch sizes, and other hyperparameters. You can choose the best hyperparameters by basically just trying many of them and choosing the best one. Let's do this together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a learning rate and batch size for SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training can take long time\n",
    "\n",
    "train_accuracies = {}  # batch size, lr -> accuracy\n",
    "test_accuracies = {}   # batch size, lr -> accuracy\n",
    "\n",
    "for learning_rate in [0.001, 0.01, 0.1]:\n",
    "    for batch_size in [32, 128, 512]:\n",
    "        model = LeNet5()\n",
    "        training_loader = torch.utils.data.DataLoader(\n",
    "            training_set, \n",
    "            batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        train(model, 100, training_loader, torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9))\n",
    "        train_accuracies[(batch_size, learning_rate)] = accuracy(model, training_loader)\n",
    "        test_accuracies[(batch_size, learning_rate)] = accuracy(model, test_loader)\n",
    "        print(\"Train accuracy:\", accuracy(model, training_loader))\n",
    "        print(\"Test accuracy:\", accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "batch_sizes = [32, 128, 512]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=len(batch_sizes), figsize=(14, 4))\n",
    "\n",
    "for batch_size, ax in zip(batch_sizes, axes):\n",
    "    aggregated_test_acc = []\n",
    "    aggregated_train_acc = []\n",
    "    for learning_rate in learning_rates:\n",
    "        aggregated_test_acc += [test_accuracies[(batch_size, learning_rate)]]\n",
    "        aggregated_train_acc += [train_accuracies[(batch_size, learning_rate)]]\n",
    "    ax.semilogx(learning_rates, aggregated_train_acc, \"o\", label = \"Train accuracy\")\n",
    "    ax.semilogx(learning_rates, aggregated_test_acc, \"o\", label = \"Test accuracy\")\n",
    "    ax.set_xlabel(\"Learning rate\")\n",
    "    ax.title.set_text(f\"Batch size {batch_size}\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explore an alternative application of optimization in Deep Learning. You will implement the 'Neural Style Transfer' algorithm by [Gatys et al. 2015](https://arxiv.org/pdf/1508.06576.pdf). The goal of this algorithm is to combine two imagines in a semantically interesting way: we will generate an image that has the 'content' of a 'contet image', but looks like the 'style' of an 'style image'.\n",
    "\n",
    "The following image from the paper illustrates how the city of Tübingen (A) can be transformed as if it were painted by several famous painters:\n",
    "![Example from Gatys et al. 2015](styletransfer-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is based on a [PyTorch tutorial](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html) by Alexis Jacq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'paintings' above are the output of an optimization algorithm. The pixels of the images are initialized as the 'content image', and optimized according to a custom loss function that simultaneously matches the style of another image—the style image—while preserving the 'content' from the oringinal image. The target is to find an 'image' that minimizes\n",
    "$$ \\textbf{loss}\\,(\\text{image} \\,|\\, \\text{content image}, \\text{style image}) \\;=\\; \\textbf{content loss}\\,(\\text{image} \\,|\\, \\text{content image}) + \\lambda \\; \\textbf{style loss}\\,(\\text{image} \\,|\\, \\text{style image}),$$\n",
    "where $\\lambda$ is a weighting parameter that is tuned experimentally.\n",
    "\n",
    "#### Content loss\n",
    "\n",
    "To match the 'semantic' content of two images while ignoring their style, Gatys et al. realize that the activation values of a **pre-trained** CNN in later layers (e.g. the 3rd convolutional layer) contain relevant information in a meaningful space. They set the content loss as the Mean Squared Error between the intermediate network activations obtained by the two inputs in those layers.\n",
    "\n",
    "#### Style loss\n",
    "\n",
    "What separates the 'content' of an image from its 'style', is that 'style' is **not localized**. Style is about which patterns appear in an image, not where they appear. This is captured by the style loss. The style of two images is again evaluated by comparing the activations of a pre-trained CNN. In this case, however, we just compare the covariance matrices of the activations in each layer. For each CNN layer, its activations consist of a feature vector for each spatial location (pixel). The covariance matrices are computed over all such vectors in a layer. The style loss follows as the MSE between covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example images\n",
    "We start by loading a bunch of images from [Unsplash](https://unsplash.com/) to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Loading\n",
    "images = tutorial.load_styletransfer_images()\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(5,14, figsize=(18, 8))\n",
    "for i, (image, ax) in enumerate(zip(images, axes.flatten())):\n",
    "    tutorial.show_image(image, ax=ax, title=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are PyTorch tensors on the GPU. Their shape is (batch size=1, rgb=3, height, width):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose any two images from this set as style and content images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "style_img = images[8]  # TODO: Pick images you like\n",
    "content_img = images[19]\n",
    "\n",
    "plt.figure()\n",
    "tutorial.show_image(style_img, title='Style Image')\n",
    "\n",
    "plt.figure()\n",
    "tutorial.show_image(content_img, title='Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified VGG network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modified the VGG19 model to return intermediate activations instead of image class predictions. Note how the `forward()` method runs the layer one by one, and collects their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VGGActivations(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This is a 'prefix' (first few layers) of the VGG19 network.\n",
    "        Instead of caring about the class predictions VGG produces for an image,\n",
    "        we are just interested in the intermediate activation values between layers.\n",
    "        That's why we chopped off the prediction part of the network.\n",
    "        \"\"\"\n",
    "        super(VGGActivations, self).__init__()\n",
    "        self.layers = [\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            \n",
    "            # ... the original VGG19 has more layers\n",
    "        ]\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # These are the normalization parameters the net was trained with\n",
    "        self.normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1, 3, 1, 1)\n",
    "        self.normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1, 3, 1, 1)\n",
    "\n",
    "        # Register the layers so they are recognized by PyTorch as 'dependent modules'\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.add_module(str(i), layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Instead of the normal forward function, which returns the predictions,\n",
    "        we now keep track of the outputs (=activations) of each intermediate convolution layer.\n",
    "        These activations are what we return.\n",
    "        \"\"\"\n",
    "        # Normalization\n",
    "        x = (x - self.normalization_mean) / self.normalization_std\n",
    "        \n",
    "        # We will store intermediate activation values in a dictionary\n",
    "        activations = {}\n",
    "        \n",
    "        for layer_number, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                activations[layer_number] = x  # shape: (batch size=1, num features=?, height, width)\n",
    "                \n",
    "        return activations\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Instantiate the model\n",
    "net = VGGActivations().to(device).eval()\n",
    "\n",
    "# Load pre-trained weights for the network from ImageNet\n",
    "result = net.load_state_dict(tutorial.get_vgg19_trained_weights(), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function requires the activation values of the style image and content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### EXERCISE\n",
    "#   Compute VGG activations for the style and content images, using the model we defined before\n",
    "style_activations = net(style_img)\n",
    "content_activations = net(content_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### EXERCISE\n",
    "#   Make sure these activation values are treated a constants.\n",
    "#   Delete all their gradient information. You can use tensor.detach_() for this.\n",
    "for _, tensor in style_activations.items():\n",
    "    tensor.detach_()\n",
    "for _, tensor in content_activations.items():\n",
    "    tensor.detach_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_content_loss(activations: dict):\n",
    "    \"\"\"\n",
    "    :param activations: dictionary [number] -> torch.Tensor\n",
    "    :return scalar - loss value\n",
    "    \"\"\"\n",
    "    ### EXERCISE\n",
    "    #   Implement the content loss\n",
    "    #   Compare the activation values of the 4th convolutional layer of VGG19 (number 7 as returned by our model)\n",
    "    #   Return the MSE between the current activations and those of the `content_img`\n",
    "    loss = 0.0\n",
    "    for layer in [7]:\n",
    "        target = content_activations[layer]\n",
    "        current = activations[layer]\n",
    "        loss += torch.nn.functional.mse_loss(current, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### EXERCISE \n",
    "#   In Neural Style Transfer, the style loss is the sum of MSE's between gram matrices at different layers.\n",
    "#   A Gram matrix G of two lists of n vectors has shape (n x n) and is defined as follows:\n",
    "#   It's entry G_ij = dot product(vector i, vector j)\n",
    "\n",
    "def gram_matrix(activations_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    :param activations_tensor: torch.Tensor of shape (1, num features, height, width)\n",
    "    :return torch.Tensor of shape (num features, num features)\n",
    "    \"\"\"\n",
    "    batch_size, n_features, height, width = activations_tensor.size()\n",
    "    assert batch_size == 1\n",
    "\n",
    "    # Todo: compute the gramm matrix G, considering the activations\n",
    "    # you can do this with matrix multiplication and reshaping (torch.view)\n",
    "    features = activations_tensor.view(n_features, height * width)  # flatten flatten width and height dimensions\n",
    "    gram_matrix = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # We 'normalize' the values of the gram matrix\n",
    "    # by dividing it by the number of elements in each feature map.\n",
    "    return gram_matrix.div(n_features * height * width)\n",
    "\n",
    "def compute_style_loss(activations: dict):\n",
    "    \"\"\"\n",
    "    :param activations: dictionary [number] -> torch.Tensor\n",
    "    :return scalar - loss value\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for layer in [0, 2, 5, 7, 10]:\n",
    "        ### EXERCISE\n",
    "        #   Compute the Mean Squared Error between the gram matrices\n",
    "        #   of the 'target' activations and the current activations (the function argument)\n",
    "        #   Add it to the 'loss' for each layer in the loop.\n",
    "        target = gram_matrix(style_activations[layer])\n",
    "        current = gram_matrix(activations[layer])\n",
    "        loss += torch.nn.functional.mse_loss(current, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_losses(activations: dict, mix_weight = 1000000):\n",
    "    content_loss = compute_content_loss(activations)\n",
    "    style_loss = compute_style_loss(activations)\n",
    "    total_loss = content_loss + mix_weight * style_loss\n",
    "    return total_loss, mix_weight * style_loss, content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the image to be optimized as the content image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image = content_img.clone()\n",
    "image.requires_grad_()  # Mark that gradients should be computed for this tensor\n",
    "\n",
    "plt.figure()\n",
    "tutorial.show_image(image, title='Initial image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### EXERCISE\n",
    "#   Optimize the image according to the neural style transfer loss\n",
    "\n",
    "# TODO: Choose an optimizer to optimize the 'image' tensor. \n",
    "#       You could try SGD with 0.9 Nesterov momentum and a lr of 0.001\n",
    "#       or LBFGS (used in original tutorial, gives good results but is more difficult to implement -- https://pytorch.org/docs/stable/_modules/torch/optim/lbfgs.html)\n",
    "#       Feel free to experiment with this.\n",
    "#       How consistent are the outcomes?\n",
    "optimizer = torch.optim.SGD([image], lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "plot = tutorial.LivePlot(legend=['Content loss', 'Style loss', 'Total loss'])\n",
    "\n",
    "for step in range(500):  # or as long as you have patience\n",
    "    activations = net(image)\n",
    "    loss, style_loss, content_loss = compute_losses(activations)\n",
    "    plot.add_point(step, content_loss.item(), style_loss.item(), loss.item())\n",
    "\n",
    "    # TODO: Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO: Project the image to stay in the range 0–1 (hint: use tensor.clamp or tensor.data.clamp_)\n",
    "    image.data.clamp_(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "tutorial.show_image(image, title='Optimized')\n",
    "plt.figure()\n",
    "tutorial.show_image(style_img, title='Style image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS([image])\n",
    "\n",
    "plot = tutorial.LivePlot(legend=['Content loss', 'Style loss', 'Total loss'])\n",
    "\n",
    "steps = 0\n",
    "while steps < 300:\n",
    "    def closure():\n",
    "        global steps\n",
    "        image.data.clamp_(0, 1)\n",
    "        activations = net(image)\n",
    "        loss, style_loss, content_loss = compute_losses(activations)\n",
    "        plot.add_point(steps, content_loss.item(), style_loss.item(), loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "out = image.data.clamp_(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
